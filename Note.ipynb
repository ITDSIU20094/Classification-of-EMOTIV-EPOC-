{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a494a20d",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5465921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db737839",
   "metadata": {},
   "source": [
    "# Data Ingestion from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34053be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and unzipping all files from the dataset...\n",
      "Dataset URL: https://www.kaggle.com/datasets/fabriciotorquato/eeg-data-from-hands-movement\n",
      "All files downloaded and unzipped to: ./kaggle2_downloads\n"
     ]
    }
   ],
   "source": [
    "# Initialize Kaggle API\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Dataset path\n",
    "kaggle_dataset = 'fabriciotorquato/eeg-data-from-hands-movement'\n",
    "output_dir = './kaggle2_downloads'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Download and unzip all files from the Kaggle dataset (download everything at once)\n",
    "print(\"Downloading and unzipping all files from the dataset...\")\n",
    "api.dataset_download_files(kaggle_dataset, path=output_dir, force=True, unzip=True)\n",
    "print(\"All files downloaded and unzipped to:\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32f488",
   "metadata": {},
   "source": [
    "# Read dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fde0aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2880 entries, 0 to 2879\n",
      "Columns: 1024 entries, Class to AF3.77\n",
      "dtypes: float64(1024)\n",
      "memory usage: 22.5 MB\n"
     ]
    }
   ],
   "source": [
    "b = pd.read_csv(\"/home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_b.csv\")\n",
    "b.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d45c320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class        0\n",
       "AF3          0\n",
       "AF3.1        0\n",
       "AF3.2        0\n",
       "AF3.3        0\n",
       "          ... \n",
       "AF3.73    2880\n",
       "AF3.74    2880\n",
       "AF3.75    2880\n",
       "AF3.76    2880\n",
       "AF3.77    2880\n",
       "Length: 1024, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.isnull().sum()  # Check for missing values in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "903fc214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns after removing those with all missing values: Index(['Class', 'AF3', 'AF3.1', 'AF3.2', 'AF3.3', 'AF3.4', 'AF3.5', 'AF3.6',\n",
      "       'AF3.7', 'F7',\n",
      "       ...\n",
      "       'F8.6', 'F8.7', 'AF4', 'AF4.1', 'AF4.2', 'AF4.3', 'AF4.4', 'AF4.5',\n",
      "       'AF4.6', 'AF4.7'],\n",
      "      dtype='object', length=113)\n"
     ]
    }
   ],
   "source": [
    "# Remove columns with all missing values\n",
    "b = b.dropna(axis=1, how='all')\n",
    "print('Columns after removing those with all missing values:', b.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd84ae8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.to_csv(\"user_b_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c449d57b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2880, 113)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_clean = pd.read_csv(\"user_b_cleaned.csv\")\n",
    "b_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47d0599f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2880, 113)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2 = pd.read_csv(\"/home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle2_downloads/Dataset/user_b.csv\")\n",
    "b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfa35e4",
   "metadata": {},
   "source": [
    "# Pre_Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d85818f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Remove columns with missing values\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocess DataFrame: remove columns with all missing values and prepare the data\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original DataFrame\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Remove columns where all values are missing\n",
    "    processed_df = processed_df.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Check if the 'Class' column exists\n",
    "    if 'Class' not in processed_df.columns:\n",
    "        raise ValueError(\"Column 'Class' does not exist in the data after handling missing values\")\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Step 2: Rename columns\n",
    "def transform_dataframe(df):\n",
    "    \"\"\"\n",
    "    Rename columns according to the new structure\n",
    "    \"\"\"\n",
    "    # List of electrodes\n",
    "    electrodes = [\n",
    "        'AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', \n",
    "        'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4'\n",
    "    ]\n",
    "\n",
    "    # Attributes for each electrode\n",
    "    attributes = ['delta std', 'delta m', 'theta std', 'theta m', \n",
    "                  'alpha std', 'alpha m', 'beta std', 'beta m']\n",
    "\n",
    "    # Create new column list\n",
    "    new_columns = ['Class']\n",
    "\n",
    "    # Create new DataFrame\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['Class'] = df['Class']\n",
    "\n",
    "    # Get the list of remaining columns (excluding Class)\n",
    "    remaining_columns = [col for col in df.columns if col != 'Class']\n",
    "    \n",
    "    # Check if the number of columns matches expectation\n",
    "    expected_electrode_columns = len(electrodes) * 8\n",
    "    if len(remaining_columns) != expected_electrode_columns:\n",
    "        print(f\"Warning: Number of columns after processing ({len(remaining_columns)}) \" \n",
    "              f\"does not match expected ({expected_electrode_columns})\")\n",
    "        print(\"Continue processing with the current number of columns...\")\n",
    "\n",
    "    # Process each electrode\n",
    "    electrode_index = 0\n",
    "    col_index = 0\n",
    "    \n",
    "    while col_index < len(remaining_columns) and electrode_index < len(electrodes):\n",
    "        electrode = electrodes[electrode_index]\n",
    "        \n",
    "        # Get the next 8 columns for the current electrode\n",
    "        if col_index + 8 <= len(remaining_columns):\n",
    "            electrode_data = df.iloc[:, df.columns.get_loc(remaining_columns[col_index]):df.columns.get_loc(remaining_columns[col_index]) + 8]\n",
    "            \n",
    "            # Ensure exactly 8 columns are selected\n",
    "            if len(electrode_data.columns) == 8:\n",
    "                # Rename columns\n",
    "                electrode_data.columns = [f\"{electrode} {attr}\" for attr in attributes]\n",
    "                \n",
    "                # Concatenate to the new DataFrame\n",
    "                new_df = pd.concat([new_df, electrode_data], axis=1)\n",
    "                \n",
    "                col_index += 8\n",
    "                electrode_index += 1\n",
    "            else:\n",
    "                print(f\"Warning: Not enough 8 columns for electrode {electrode}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Warning: Not enough columns for electrode {electrode}\")\n",
    "            break\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Main function to process all steps\n",
    "def process_eeg_data(input_df):\n",
    "    \"\"\"\n",
    "    Main function to process EEG data\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocessing - remove columns with missing values\n",
    "    print(\"Step 1: Remove columns with missing values...\")\n",
    "    processed_df = preprocess_dataframe(input_df)\n",
    "    print(f\"Number of columns after handling missing values: {len(processed_df.columns)}\")\n",
    "    \n",
    "    # Step 2: Rename columns\n",
    "    print(\"Step 2: Rename columns...\")\n",
    "    final_df = transform_dataframe(processed_df)\n",
    "    print(f\"Final number of columns: {len(final_df.columns)}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# Usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# result_df = process_eeg_data(df)\n",
    "# result_df.to_csv('processed_data.csv', index=False)\n",
    "\n",
    "# Or if you want to see information about the data:\n",
    "# print(\"Original DataFrame info:\")\n",
    "# print(f\"Number of columns: {len(df.columns)}\")\n",
    "# print(f\"Columns: {list(df.columns)}\")\n",
    "# \n",
    "# result_df = process_eeg_data(df)\n",
    "# print(\"\\nDataFrame info after processing:\")\n",
    "# print(f\"Number of columns: {len(result_df.columns)}\")\n",
    "# print(f\"First 5 columns: {list(result_df.columns[:5])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f79725a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Remove columns with missing values...\n",
      "Number of columns after handling missing values: 113\n",
      "Step 2: Rename columns...\n",
      "Final number of columns: 113\n",
      "Original DataFrame info:\n",
      "Number of columns: 113\n",
      "Columns: ['Class', 'AF3', 'AF3.1', 'AF3.2', 'AF3.3', 'AF3.4', 'AF3.5', 'AF3.6', 'AF3.7', 'F7', 'F7.1', 'F7.2', 'F7.3', 'F7.4', 'F7.5', 'F7.6', 'F7.7', 'F3', 'F3.1', 'F3.2', 'F3.3', 'F3.4', 'F3.5', 'F3.6', 'F3.7', 'FC5', 'FC5.1', 'FC5.2', 'FC5.3', 'FC5.4', 'FC5.5', 'FC5.6', 'FC5.7', 'T7', 'T7.1', 'T7.2', 'T7.3', 'T7.4', 'T7.5', 'T7.6', 'T7.7', 'P7', 'P7.1', 'P7.2', 'P7.3', 'P7.4', 'P7.5', 'P7.6', 'P7.7', 'O1', 'O1.1', 'O1.2', 'O1.3', 'O1.4', 'O1.5', 'O1.6', 'O1.7', 'O2', 'O2.1', 'O2.2', 'O2.3', 'O2.4', 'O2.5', 'O2.6', 'O2.7', 'P8', 'P8.1', 'P8.2', 'P8.3', 'P8.4', 'P8.5', 'P8.6', 'P8.7', 'T8', 'T8.1', 'T8.2', 'T8.3', 'T8.4', 'T8.5', 'T8.6', 'T8.7', 'FC6', 'FC6.1', 'FC6.2', 'FC6.3', 'FC6.4', 'FC6.5', 'FC6.6', 'FC6.7', 'F4', 'F4.1', 'F4.2', 'F4.3', 'F4.4', 'F4.5', 'F4.6', 'F4.7', 'F8', 'F8.1', 'F8.2', 'F8.3', 'F8.4', 'F8.5', 'F8.6', 'F8.7', 'AF4', 'AF4.1', 'AF4.2', 'AF4.3', 'AF4.4', 'AF4.5', 'AF4.6', 'AF4.7']\n",
      "Shape of original DataFrame: (2880, 113)\n",
      "\n",
      "DataFrame info after processing:\n",
      "Number of columns: 113\n",
      "First 5 columns: ['Class', 'AF3 delta std', 'AF3 delta m', 'AF3 theta std', 'AF3 theta m']\n",
      "Shape of processed DataFrame: (2880, 113)\n"
     ]
    }
   ],
   "source": [
    "b = pd.read_csv(\"/home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_a.csv\")\n",
    "b_cleaned = process_eeg_data(b)\n",
    "print(\"Original DataFrame info:\")\n",
    "print(f\"Number of columns: {len(b.columns)}\")\n",
    "print(f\"Columns: {list(b.columns)}\")\n",
    "print(\"Shape of original DataFrame:\", b.shape)\n",
    "\n",
    "print(\"\\nDataFrame info after processing:\")\n",
    "print(f\"Number of columns: {len(b_cleaned.columns)}\")\n",
    "print(f\"First 5 columns: {list(b_cleaned.columns[:5])}\")\n",
    "print(\"Shape of processed DataFrame:\", b_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37be3fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing: /home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_a.csv\n",
      "Original columns: 113\n",
      "Processing file: user_a.csv\n",
      "Step 1: Removing columns with missing values...\n",
      "Columns after missing values processing: 113\n",
      "Step 2: Transforming column names...\n",
      "Final number of columns: 113\n",
      "Successfully processed and saved to: data/user_a.csv\n",
      "\n",
      "==================================================\n",
      "Processing: /home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_b.csv\n",
      "Original columns: 1024\n",
      "Processing file: user_b.csv\n",
      "Step 1: Removing columns with missing values...\n",
      "Columns after missing values processing: 113\n",
      "Step 2: Transforming column names...\n",
      "Final number of columns: 113\n",
      "Successfully processed and saved to: data/user_a.csv\n",
      "\n",
      "==================================================\n",
      "Processing: /home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_b.csv\n",
      "Original columns: 1024\n",
      "Processing file: user_b.csv\n",
      "Step 1: Removing columns with missing values...\n",
      "Columns after missing values processing: 113\n",
      "Step 2: Transforming column names...\n",
      "Final number of columns: 113\n",
      "Successfully processed and saved to: data/user_b.csv\n",
      "\n",
      "==================================================\n",
      "Processing: /home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_c.csv\n",
      "Original columns: 1024\n",
      "Processing file: user_c.csv\n",
      "Step 1: Removing columns with missing values...\n",
      "Columns after missing values processing: 113\n",
      "Step 2: Transforming column names...\n",
      "Final number of columns: 113\n",
      "Successfully processed and saved to: data/user_b.csv\n",
      "\n",
      "==================================================\n",
      "Processing: /home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_c.csv\n",
      "Original columns: 1024\n",
      "Processing file: user_c.csv\n",
      "Step 1: Removing columns with missing values...\n",
      "Columns after missing values processing: 113\n",
      "Step 2: Transforming column names...\n",
      "Final number of columns: 113\n",
      "Successfully processed and saved to: data/user_c.csv\n",
      "\n",
      "==================================================\n",
      "Processing: /home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_d.csv\n",
      "Original columns: 1024\n",
      "Processing file: user_d.csv\n",
      "Step 1: Removing columns with missing values...\n",
      "Columns after missing values processing: 113\n",
      "Step 2: Transforming column names...\n",
      "Final number of columns: 113\n",
      "Successfully processed and saved to: data/user_c.csv\n",
      "\n",
      "==================================================\n",
      "Processing: /home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_d.csv\n",
      "Original columns: 1024\n",
      "Processing file: user_d.csv\n",
      "Step 1: Removing columns with missing values...\n",
      "Columns after missing values processing: 113\n",
      "Step 2: Transforming column names...\n",
      "Final number of columns: 113\n",
      "Successfully processed and saved to: data/user_d.csv\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "==================================================\n",
      "user_a.csv: SUCCESS (113 → 113 columns)\n",
      "user_b.csv: SUCCESS (1024 → 113 columns)\n",
      "user_c.csv: SUCCESS (1024 → 113 columns)\n",
      "user_d.csv: SUCCESS (1024 → 113 columns)\n",
      "Successfully processed and saved to: data/user_d.csv\n",
      "\n",
      "==================================================\n",
      "PROCESSING SUMMARY:\n",
      "==================================================\n",
      "user_a.csv: SUCCESS (113 → 113 columns)\n",
      "user_b.csv: SUCCESS (1024 → 113 columns)\n",
      "user_c.csv: SUCCESS (1024 → 113 columns)\n",
      "user_d.csv: SUCCESS (1024 → 113 columns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# List of electrodes\n",
    "electrodes = [\n",
    "    'AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', \n",
    "    'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4'\n",
    "]\n",
    "\n",
    "# Attributes for each electrode\n",
    "attributes = ['delta std', 'delta m', 'theta std', 'theta m', \n",
    "              'alpha std', 'alpha m', 'beta std', 'beta m']\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Preprocess DataFrame: remove columns with all missing values\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original DataFrame\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Remove columns with all missing values\n",
    "    processed_df = processed_df.dropna(axis=1, how='all')\n",
    "    \n",
    "    # Check if Class column exists\n",
    "    if 'Class' not in processed_df.columns:\n",
    "        raise ValueError(\"'Class' column does not exist in data after missing values processing\")\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "def transform_dataframe(df):\n",
    "    \"\"\"\n",
    "    Transform column names to new structure\n",
    "    \"\"\"\n",
    "    # Create new DataFrame\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df['Class'] = df['Class']\n",
    "\n",
    "    # Get remaining columns (excluding Class)\n",
    "    remaining_columns = [col for col in df.columns if col != 'Class']\n",
    "    \n",
    "    # Check if column count matches expected\n",
    "    expected_electrode_columns = len(electrodes) * 8\n",
    "    if len(remaining_columns) != expected_electrode_columns:\n",
    "        print(f\"Warning: Number of columns after processing ({len(remaining_columns)}) \" \n",
    "              f\"does not match expected ({expected_electrode_columns})\")\n",
    "        print(\"Continuing processing with current column count...\")\n",
    "\n",
    "    # Process each electrode\n",
    "    electrode_index = 0\n",
    "    col_index = 0\n",
    "    \n",
    "    while col_index < len(remaining_columns) and electrode_index < len(electrodes):\n",
    "        electrode = electrodes[electrode_index]\n",
    "        \n",
    "        # Get next 8 columns for current electrode\n",
    "        if col_index + 8 <= len(remaining_columns):\n",
    "            # Get the actual column indices\n",
    "            start_col_name = remaining_columns[col_index]\n",
    "            end_col_name = remaining_columns[col_index + 7] if col_index + 7 < len(remaining_columns) else remaining_columns[-1]\n",
    "            \n",
    "            # Get column indices\n",
    "            start_idx = df.columns.get_loc(start_col_name)\n",
    "            end_idx = df.columns.get_loc(end_col_name) + 1\n",
    "            \n",
    "            electrode_data = df.iloc[:, start_idx:end_idx]\n",
    "            \n",
    "            # Ensure we have exactly 8 columns\n",
    "            if len(electrode_data.columns) == 8:\n",
    "                # Rename columns\n",
    "                electrode_data.columns = [f\"{electrode} {attr}\" for attr in attributes]\n",
    "                \n",
    "                # Concatenate to new DataFrame\n",
    "                new_df = pd.concat([new_df, electrode_data], axis=1)\n",
    "                \n",
    "                col_index += 8\n",
    "                electrode_index += 1\n",
    "            else:\n",
    "                print(f\"Warning: Not enough columns for electrode {electrode}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Warning: Not enough columns for electrode {electrode}\")\n",
    "            break\n",
    "\n",
    "    return new_df\n",
    "\n",
    "def process_eeg_data(input_df, file_name=\"\"):\n",
    "    \"\"\"\n",
    "    Main function to process EEG data\n",
    "    \"\"\"\n",
    "    if file_name:\n",
    "        print(f\"Processing file: {file_name}\")\n",
    "    \n",
    "    # Step 1: Preprocessing - remove columns with missing values\n",
    "    print(\"Step 1: Removing columns with missing values...\")\n",
    "    processed_df = preprocess_dataframe(input_df)\n",
    "    print(f\"Columns after missing values processing: {len(processed_df.columns)}\")\n",
    "    \n",
    "    # Step 2: Transform column names\n",
    "    print(\"Step 2: Transforming column names...\")\n",
    "    final_df = transform_dataframe(processed_df)\n",
    "    print(f\"Final number of columns: {len(final_df.columns)}\")\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def process_multiple_files(file_patterns, output_folder=\"processed_data\"):\n",
    "    \"\"\"\n",
    "    Process multiple CSV files simultaneously\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    Path(output_folder).mkdir(exist_ok=True)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Process each file\n",
    "    for file_pattern in file_patterns:\n",
    "        # Find files matching the pattern\n",
    "        file_paths = glob.glob(file_pattern)\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Processing: {file_path}\")\n",
    "                \n",
    "                # Read CSV file\n",
    "                df = pd.read_csv(file_path)\n",
    "                print(f\"Original columns: {len(df.columns)}\")\n",
    "                \n",
    "                # Process the data\n",
    "                processed_df = process_eeg_data(df, Path(file_path).name)\n",
    "                \n",
    "                # Save processed file\n",
    "                output_path = Path(output_folder) / f\"{Path(file_path).name}\"\n",
    "                processed_df.to_csv(output_path, index=False)\n",
    "                \n",
    "                results[file_path] = {\n",
    "                    'status': 'success',\n",
    "                    'output_path': str(output_path),\n",
    "                    'original_columns': len(df.columns),\n",
    "                    'processed_columns': len(processed_df.columns)\n",
    "                }\n",
    "                \n",
    "                print(f\"Successfully processed and saved to: {output_path}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {str(e)}\")\n",
    "                results[file_path] = {\n",
    "                    'status': 'error',\n",
    "                    'error_message': str(e)\n",
    "                }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Define your file patterns (can use wildcards)\n",
    "    file_patterns = [\n",
    "        \"/home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_a.csv\",\n",
    "        \"/home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_b.csv\", \n",
    "        \"/home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_c.csv\",\n",
    "        \"/home/quan/PROJECT/Machine Learning with Biomedical Signals/kaggle_downloads/user_d.csv\"\n",
    "    ]\n",
    "    \n",
    "    # Alternative: use wildcard pattern to match multiple files\n",
    "    # file_patterns = [\"data_*.csv\"]  # This will process all files starting with \"data_\"\n",
    "    \n",
    "    # Process all files\n",
    "    processing_results = process_multiple_files(file_patterns, output_folder=\"cleaned_data\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"PROCESSING SUMMARY:\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for file_path, result in processing_results.items():\n",
    "        if result['status'] == 'success':\n",
    "            print(f\"{Path(file_path).name}: SUCCESS \"\n",
    "                  f\"({result['original_columns']} → {result['processed_columns']} columns)\")\n",
    "        else:\n",
    "            print(f\"{Path(file_path).name}: ERROR - {result['error_message']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
